{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spam Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to bulid a spam classifier using Naive Bayes Classification with Laplace Smoothening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "X=np.genfromtxt('.//spam_X.csv',dtype='str',delimiter='\\n',skip_header=0,invalid_raise=True)\n",
    "Y=np.genfromtxt('.//spam_Y.csv',dtype='str',delimiter='\\n')\n",
    "Y=np.where(Y=='1','0','1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4296,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset consists of 4296 Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class '0' denotes 'spam' Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class '1' denotes 'non-spam' Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing the feature sentences are\n",
      "\n",
      "\"Subject: re : faculty lunch  alison ,  i recommended inviting duane seppi and steven shreve . i would also invite  brian routledge .  i don ' t know him but heard many good things about him . kevin kindall may have  other recommendations .  vince  enron north america corp .  from : mary alison bailey 09 / 08 / 2000 05 : 04 pm  to : shirley crenshaw / hou / ect @ ect , vince j kaminski / hou / ect @ ect , kevin  kuykendall / hou / ect @ ect , kevin kuykendall / hou / ect @ ect  cc : kristin gandy / na / enron @ enron  subject : faculty lunch  kristin had said she was interested in a faculty lunch , and kevin said he  would host it . are there any professors you would recommend be invited ?  here is a list of finance faculty :  robert dammon rdl 9 @ andrew . cmu . edu 412 . 268 . 3696  richard green rgob @ andrew . cmu . edu 412 . 268 . 2302  david heath heath @ andrew . cmu . edu 412 . 268 . 2545  christine parlour parlourc @ andrew . cmu . edu 412 . 268 . 5806  brian routledge rout @ andrew . cmu . edu 412 . 268 . 7588  duane seppi ds 64 @ andrew . cmu . edu 412 . 268 . 2298  steven shreve shreve @ andrew . cmu . edu 412 . 268 . 8484  chester spatt cs 9 z @ andrew . cmu . edu 412 . 268 . 8834  christopher telmer telmerc @ andrew . cmu . edu 412 . 268 . 8838  stanley zin szoh @ andrew . cmu . edu 412 . 268 . 3700  *\"\n",
      "\n",
      "\"Subject: re : interest in a position  alison ,  my group needs generally people with advanced skills in mathematics and  programming .  i shall try to help you by forwarding your resume ( with your permission ) to  other  units of enron . please , let me know if it ' s ok with you .  vince  enron north america corp .  from : mary alison bailey 11 / 21 / 2000 09 : 42 am  to : vince j kaminski / hou / ect @ ect  cc :  subject : interest in a position  dear vince ,  when we talked last , you mentioned you were considering making additions to  your office staff . if you are still considering these additions , i would  like to talk to you and see if there might be a place for me . it would be  wonderful to work with a group such as yours .  it has been great working in recruiting , but the time has come for me to try  something else . i am beginning to look around , but wanted to talk to you  first . if my skill sets match a position you might be adding , please  consider me . i have attached my resume so that you will have an idea of what  i have done .  thank you for your consideration . have a great thanksgiving !  alison bailey  713 - 853 - 6778\"\n",
      "\n",
      "\"Subject: re : implementing term - structure of correlations for power  tanya ,  while there is seasonal correlations in power , especially for np - 15  and sp - 15 ( same region ) , the term structure of correlations can be input .  however , the same correlation structure with similar periodicity may not hold  between np - 15 and , say , rlb ( neepool ) , though one would imagine that  relationship would still be seasonal ( summer / winter ) , with greater noise .  even if the correlational term structure is to be done for power , different  rules would have to be inputted for different regions .  naveen  tanya tamarchenko @ ect  10 / 05 / 2000 10 : 42 am  to : vladimir gorny / hou / ect @ ect , naveen andrews / corp / enron @ enron  cc : kirstee hewitt / lon / ect @ ect , debbie r brackett / hou / ect @ ect , wenyao  jia / hou / ect @ ect , vince j kaminski / hou / ect @ ect  subject : re : implementing term - structure of correlations for power  vlady  2 ) correlations are periodic with a period of 1 year ( this means we can use  12 correlation matrices calculated from  first 12 forward contracts and apply these matrices to other forward months ) ;  3 ) using decay factor makes the curves a little smoother .  implementation of multiple correlation matrices will not affect the speed of  calculations in var model significantly .  please , give me your response ,  thanks ,  tanya .\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before preprocessing the feature sentences are\")\n",
    "print()\n",
    "print(X[0])\n",
    "print()\n",
    "print(X[200])\n",
    "print()\n",
    "print(X[4000])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the feature sentence to remove numbers and special characters\n",
    "#also converting all upper case letters to lower case\n",
    "for i in range(0,X.shape[0],1):\n",
    "    s=X[i]\n",
    "    flag=0\n",
    "    s1=\"\"\n",
    "    for j in range(0,len(s),1):\n",
    "        if(ord(s[j])>=65 and ord(s[j])<=90):\n",
    "            if(flag==1 and len(s1)>0):\n",
    "                s1=s1+\" \"\n",
    "            s1=s1+chr(ord(s[j])+32)\n",
    "            flag=0\n",
    "        elif(ord(s[j])>=97 and ord(s[j])<=122):\n",
    "            if(flag==1 and len(s1)>0):\n",
    "                s1=s1+\" \"\n",
    "            s1=s1+s[j]\n",
    "            flag=0\n",
    "        elif(s[j]==' '):\n",
    "            flag=1\n",
    "        elif(s[j]==\"'\"):\n",
    "            continue\n",
    "        else:\n",
    "            flag=1\n",
    "    X[i]=s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing the feature sentences are\n",
      "\n",
      "subject re faculty lunch alison i recommended inviting duane seppi and steven shreve i would also invite brian routledge i don t know him but heard many good things about him kevin kindall may have other recommendations vince enron north america corp from mary alison bailey pm to shirley crenshaw hou ect ect vince j kaminski hou ect ect kevin kuykendall hou ect ect kevin kuykendall hou ect ect cc kristin gandy na enron enron subject faculty lunch kristin had said she was interested in a faculty lunch and kevin said he would host it are there any professors you would recommend be invited here is a list of finance faculty robert dammon rdl andrew cmu edu richard green rgob andrew cmu edu david heath heath andrew cmu edu christine parlour parlourc andrew cmu edu brian routledge rout andrew cmu edu duane seppi ds andrew cmu edu steven shreve shreve andrew cmu edu chester spatt cs z andrew cmu edu christopher telmer telmerc andrew cmu edu stanley zin szoh andrew cmu edu\n",
      "\n",
      "subject re interest in a position alison my group needs generally people with advanced skills in mathematics and programming i shall try to help you by forwarding your resume with your permission to other units of enron please let me know if it s ok with you vince enron north america corp from mary alison bailey am to vince j kaminski hou ect ect cc subject interest in a position dear vince when we talked last you mentioned you were considering making additions to your office staff if you are still considering these additions i would like to talk to you and see if there might be a place for me it would be wonderful to work with a group such as yours it has been great working in recruiting but the time has come for me to try something else i am beginning to look around but wanted to talk to you first if my skill sets match a position you might be adding please consider me i have attached my resume so that you will have an idea of what i have done thank you for your consideration have a great thanksgiving alison bailey\n",
      "\n",
      "subject re implementing term structure of correlations for power tanya while there is seasonal correlations in power especially for np and sp same region the term structure of correlations can be input however the same correlation structure with similar periodicity may not hold between np and say rlb neepool though one would imagine that relationship would still be seasonal summer winter with greater noise even if the correlational term structure is to be done for power different rules would have to be inputted for different regions naveen tanya tamarchenko ect am to vladimir gorny hou ect ect naveen andrews corp enron enron cc kirstee hewitt lon ect ect debbie r brackett hou ect ect wenyao jia hou ect ect vince j kaminski hou ect ect subject re implementing term structure of correlations for power vlady correlations are periodic with a period of year this means we can use correlation matrices calculated from first forward contracts and apply these matrices to other forward months using decay factor makes the curves a little smoother implementation of multiple correlation matrices will not affect the speed of calculations in var model significantly please give me your response thanks tanya\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"After preprocessing the feature sentences are\")\n",
    "print()\n",
    "print(X[0])\n",
    "print()\n",
    "print(X[200])\n",
    "print()\n",
    "print(X[4000])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into training data and testing data\n",
    "train_X=X[:int(X.shape[0]*0.8)]\n",
    "train_Y=Y[:int(Y.shape[0]*0.8)]\n",
    "test_X=X[int(X.shape[0]*0.8):]\n",
    "test_Y=Y[int(Y.shape[0]*0.8):]\n",
    "m=train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation Phase 1\n",
      "For alpha = 1\n",
      "F1_score = 0.6362735936162235\n",
      "For alpha = 3\n",
      "F1_score = 0.6394132752687111\n",
      "For alpha = 5\n",
      "F1_score = 0.6411832338569231\n",
      "For alpha = 10\n",
      "F1_score = 0.6472267615502028\n",
      "For alpha = 30\n",
      "F1_score = 0.6210224083881212\n",
      "For alpha = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score = nan\n",
      "Cross validation Phase 2\n",
      "For alpha = 1\n",
      "F1_score = 0.6177830684693849\n",
      "For alpha = 3\n",
      "F1_score = 0.6196392253957026\n",
      "For alpha = 5\n",
      "F1_score = 0.6280316374744345\n",
      "For alpha = 10\n",
      "F1_score = 0.6364865330572169\n",
      "For alpha = 30\n",
      "F1_score = nan\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 3\n",
      "For alpha = 1\n",
      "F1_score = 0.6501457725947521\n",
      "For alpha = 3\n",
      "F1_score = 0.6536465661830093\n",
      "For alpha = 5\n",
      "F1_score = 0.6478472986733134\n",
      "For alpha = 10\n",
      "F1_score = 0.6580010095911156\n",
      "For alpha = 30\n",
      "F1_score = 0.6382658019476013\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 4\n",
      "For alpha = 1\n",
      "F1_score = 0.6334633020571286\n",
      "For alpha = 3\n",
      "F1_score = 0.6402147288673135\n",
      "For alpha = 5\n",
      "F1_score = 0.6381455198358413\n",
      "For alpha = 10\n",
      "F1_score = 0.625818559124188\n",
      "For alpha = 30\n",
      "F1_score = 0.6295779833632381\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 5\n",
      "For alpha = 1\n",
      "F1_score = 0.5808094465476834\n",
      "For alpha = 3\n",
      "F1_score = 0.5974105120955345\n",
      "For alpha = 5\n",
      "F1_score = 0.609551690750653\n",
      "For alpha = 10\n",
      "F1_score = 0.6220983931595688\n",
      "For alpha = 30\n",
      "F1_score = 0.6376432624902637\n",
      "For alpha = 50\n",
      "F1_score = 0.6310710030448788\n",
      "For alpha = 100\n",
      "F1_score = 0.6310710030448788\n",
      "For alpha = 300\n",
      "F1_score = 0.6310710030448788\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 6\n",
      "For alpha = 1\n",
      "F1_score = 0.5794639038587859\n",
      "For alpha = 3\n",
      "F1_score = 0.5903471536362932\n",
      "For alpha = 5\n",
      "F1_score = 0.5942706504123524\n",
      "For alpha = 10\n",
      "F1_score = 0.6126411499374775\n",
      "For alpha = 30\n",
      "F1_score = nan\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 7\n",
      "For alpha = 1\n",
      "F1_score = 0.630989445692389\n",
      "For alpha = 3\n",
      "F1_score = 0.6373115745475565\n",
      "For alpha = 5\n",
      "F1_score = 0.6635267355448391\n",
      "For alpha = 10\n",
      "F1_score = 0.6659023910907305\n",
      "For alpha = 30\n",
      "F1_score = 0.647096442045482\n",
      "For alpha = 50\n",
      "F1_score = 0.6391793326482881\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 8\n",
      "For alpha = 1\n",
      "F1_score = 0.6570759767971074\n",
      "For alpha = 3\n",
      "F1_score = 0.6719781555063771\n",
      "For alpha = 5\n",
      "F1_score = 0.6598730920939806\n",
      "For alpha = 10\n",
      "F1_score = 0.6498961880474351\n",
      "For alpha = 30\n",
      "F1_score = nan\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 9\n",
      "For alpha = 1\n",
      "F1_score = 0.5724163418340381\n",
      "For alpha = 3\n",
      "F1_score = 0.5793841171147376\n",
      "For alpha = 5\n",
      "F1_score = 0.6025604553592525\n",
      "For alpha = 10\n",
      "F1_score = 0.6102194562725188\n",
      "For alpha = 30\n",
      "F1_score = nan\n",
      "For alpha = 50\n",
      "F1_score = nan\n",
      "For alpha = 100\n",
      "F1_score = nan\n",
      "For alpha = 300\n",
      "F1_score = nan\n",
      "For alpha = 500\n",
      "F1_score = nan\n",
      "Cross validation Phase 10\n",
      "For alpha = 1\n",
      "F1_score = 0.6327434764063521\n",
      "For alpha = 3\n",
      "F1_score = 0.633207696243192\n",
      "For alpha = 5\n",
      "F1_score = 0.6317141706212359\n",
      "For alpha = 10\n",
      "F1_score = 0.6522479445074962\n",
      "For alpha = 30\n",
      "F1_score = 0.6487565622108338\n",
      "For alpha = 50\n",
      "F1_score = 0.6391793326482881\n",
      "For alpha = 100\n",
      "F1_score = 0.632560514654488\n",
      "For alpha = 300\n",
      "F1_score = 0.632560514654488\n",
      "For alpha = 500\n",
      "F1_score = nan\n"
     ]
    }
   ],
   "source": [
    "#Using 10 fold cross validation to tune the hyper parameter alpha\n",
    "alpha=[]\n",
    "for jj in range(0,10,1):\n",
    "    print(\"Cross validation Phase\",jj+1)\n",
    "    val_x=train_X[int(jj*m*0.1):int((jj+1)*m*0.1)]\n",
    "    val_y=train_Y[int(jj*m*0.1):int((jj+1)*m*0.1)]\n",
    "    train_x=np.hstack((train_X[:int(jj*m*0.1)],train_X[int((jj+1)*m*0.1):]))\n",
    "    train_y=np.hstack((train_Y[:int(jj*m*0.1)],train_Y[int((jj+1)*m*0.1):]))\n",
    "    #using the training data to form a class wise dictionary containing words that appear in a particular class as keys and their frequencies as values\n",
    "    d={}\n",
    "    for i in range(0,train_x.shape[0],1):\n",
    "        if(train_y[i] in d.keys()):\n",
    "            for j in train_x[i].split(\" \"):\n",
    "                if(j in d[train_y[i]].keys()):\n",
    "                    d[train_y[i]][j]+=1\n",
    "                else:\n",
    "                    d[train_y[i]][j]=1\n",
    "        else:\n",
    "            d[train_y[i]]={}\n",
    "            for j in train_x[i].split(\" \"):\n",
    "                if(j in d[train_y[i]].keys()):\n",
    "                    d[train_y[i]][j]+=1\n",
    "                else:\n",
    "                    d[train_y[i]][j]=1\n",
    "    class_wise_words_dict=d\n",
    "    #using the training data to compute the prior probability of each class\n",
    "    d={}\n",
    "    u=np.unique(train_y,return_counts=True)\n",
    "    s=np.sum(u[1])\n",
    "    for i in range(0,len(u[0]),1):\n",
    "        d[u[0][i]]=u[1][i]/s\n",
    "    prior_prob=d\n",
    "    #using the validation data to tune the hyperparameter alpha\n",
    "    accuracy=0\n",
    "    a=[1,3,5,10,30,50,100,300,500]\n",
    "    classes=np.unique(train_y)\n",
    "    for alph in a:\n",
    "        #computing the denominator of the liklihood for each class\n",
    "        print(\"For alpha =\",alph)\n",
    "        denominator_d={}\n",
    "        t_s=0\n",
    "        for i in class_wise_words_dict:\n",
    "            s=0\n",
    "            for j in class_wise_words_dict[i]:\n",
    "                s=s+class_wise_words_dict[i][j]\n",
    "                t_s=t_s+1\n",
    "            denominator_d[i]=s\n",
    "        for i in denominator_d:\n",
    "            denominator_d[i]=denominator_d[i]+alph*t_s\n",
    "        class_wise_denominators_dict=denominator_d\n",
    "        #predicting the class for the validation data using naive bayes algorithm\n",
    "        y_pred=[]\n",
    "        for i in range(0,val_x.shape[0],1):\n",
    "            prob=0\n",
    "            for c in classes:\n",
    "                p=0\n",
    "                for j in val_x[i].split(\" \"):\n",
    "                    if(j in class_wise_words_dict[c].keys()):\n",
    "                        p=p+np.log((class_wise_words_dict[c][j]+alph)/class_wise_denominators_dict[c])\n",
    "                    else:\n",
    "                        p=p+np.log(alph/class_wise_denominators_dict[c])\n",
    "                p=p+np.log(prior_prob[c])\n",
    "                if(p>prob or prob==0):\n",
    "                    prob=p\n",
    "                    cl=c\n",
    "            y_pred=y_pred+[cl]\n",
    "        #evaluating the model using the weighted F1_score\n",
    "        f1=0\n",
    "        c_m=np.zeros((len(classes),len(classes)),dtype='int64')\n",
    "        for i in range(0,len(y_pred),1):\n",
    "            c_m[int(y_pred[i]),int(test_Y[i])]+=1\n",
    "        tp=np.zeros(len(classes),dtype='int64')\n",
    "        fp=np.zeros(len(classes),dtype='int64')\n",
    "        fn=np.zeros(len(classes),dtype='int64')\n",
    "        w=np.zeros(len(classes),dtype='int64')\n",
    "        pre=np.zeros(len(classes),dtype='float64')\n",
    "        rec=np.zeros(len(classes),dtype='float64')\n",
    "        F1=np.zeros(len(classes),dtype='float64')\n",
    "        F1_score=0\n",
    "        for i in classes:\n",
    "            i=int(i)\n",
    "            tp[i]=c_m[i,i]\n",
    "            fp[i]=np.sum(c_m[i,:])-tp[i]\n",
    "            fn[i]=np.sum(c_m[:,i])-tp[i]\n",
    "            w[i]=np.sum(c_m[:,i])\n",
    "            pre[i]=tp[i]/(tp[i]+fp[i])\n",
    "            rec[i]=tp[i]/(tp[i]+fn[i])\n",
    "            F1[i]=2*pre[i]*rec[i]/(pre[i]+rec[i])\n",
    "            F1_score=F1_score+F1[i]*w[i]\n",
    "        F1_score=F1_score/np.sum(w)\n",
    "        print(\"F1_score =\",F1_score)\n",
    "        if(F1_score>f1):\n",
    "            f1=F1_score\n",
    "            Alpha=alph\n",
    "    alpha+=[Alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 10\n"
     ]
    }
   ],
   "source": [
    "Alpha=np.unique(alpha,return_counts=True)\n",
    "alpha=Alpha[0][np.argmax(Alpha[1])]\n",
    "print(\"alpha =\",alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using the training and validation data\n",
    "#forming a class wise dictionary containing words that appear in a particular class as keys and their frequencies as values\n",
    "d={}\n",
    "for i in range(0,train_X.shape[0],1):\n",
    "    if(train_Y[i] in d.keys()):\n",
    "        for j in train_X[i].split(\" \"):\n",
    "            if(j in d[train_Y[i]].keys()):\n",
    "                d[train_Y[i]][j]+=1\n",
    "            else:\n",
    "                d[train_Y[i]][j]=1\n",
    "    else:\n",
    "        d[train_Y[i]]={}\n",
    "        for j in train_X[i].split(\" \"):\n",
    "            if(j in d[train_Y[i]].keys()):\n",
    "                d[train_Y[i]][j]+=1\n",
    "            else:\n",
    "                d[train_Y[i]][j]=1\n",
    "class_wise_words_dict=d\n",
    "#computing the prior probability of each class\n",
    "d={}\n",
    "u=np.unique(train_Y,return_counts=True)\n",
    "s=np.sum(u[1])\n",
    "for i in range(0,len(u[0]),1):\n",
    "    d[u[0][i]]=u[1][i]/s\n",
    "prior_prob=d\n",
    "#computing the denominator of the liklihood for each class\n",
    "denominator_d={}\n",
    "t_s=0\n",
    "for i in class_wise_words_dict:\n",
    "    s=0\n",
    "    for j in class_wise_words_dict[i]:\n",
    "        s=s+class_wise_words_dict[i][j]\n",
    "        t_s=t_s+1\n",
    "    denominator_d[i]=s\n",
    "for i in denominator_d:\n",
    "    denominator_d[i]=denominator_d[i]+alpha*t_s\n",
    "class_wise_denominators_dict=denominator_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the model to predict the labels of the test data\n",
    "y_pred=[]\n",
    "for i in range(0,test_X.shape[0],1):\n",
    "    prob=0\n",
    "    for c in classes:\n",
    "        p=0\n",
    "        for j in test_X[i].split(\" \"):\n",
    "            if(j in class_wise_words_dict[c].keys()):\n",
    "                p=p+np.log((class_wise_words_dict[c][j]+alpha)/class_wise_denominators_dict[c])\n",
    "            else:\n",
    "                p=p+np.log(alpha/class_wise_denominators_dict[c])\n",
    "        p=p+np.log(prior_prob[c])\n",
    "        if(p>prob or prob==0):\n",
    "            prob=p\n",
    "            cl=c\n",
    "    y_pred=y_pred+[cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating the confusion matrix\n",
    "c_m=np.zeros((len(classes),len(classes)),dtype='int64')\n",
    "for i in range(0,len(y_pred),1):\n",
    "    c_m[int(y_pred[i]),int(test_Y[i])]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is\n",
      "[[ 77   2]\n",
      " [139 642]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Confusion Matrix is\")\n",
    "print(c_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing the weighted F1_score of the model\n",
    "tp=np.zeros(len(classes))\n",
    "fp=np.zeros(len(classes))\n",
    "fn=np.zeros(len(classes))\n",
    "w=np.zeros(len(classes))\n",
    "pre=np.zeros(len(classes))\n",
    "rec=np.zeros(len(classes))\n",
    "F1=np.zeros(len(classes))\n",
    "F1_score=0\n",
    "for i in classes:\n",
    "    i=int(i)\n",
    "    tp[i]=c_m[i,i]\n",
    "    fp[i]=np.sum(c_m[i,:])-tp[i]\n",
    "    fn[i]=np.sum(c_m[:,i])-tp[i]\n",
    "    w[i]=np.sum(c_m[:,i])\n",
    "    pre[i]=tp[i]/(tp[i]+fp[i])\n",
    "    rec[i]=tp[i]/(tp[i]+fn[i])\n",
    "    F1[i]=2*pre[i]*rec[i]/(pre[i]+rec[i])\n",
    "    F1_score=F1_score+F1[i]*w[i]\n",
    "F1_score=F1_score/np.sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score = 0.8058572288031866\n"
     ]
    }
   ],
   "source": [
    "print(\"F1_score =\",F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.836046511627907\n"
     ]
    }
   ],
   "source": [
    "#Calculating Accuracy\n",
    "accuracy=(tp[0]+tp[1])/test_X.shape[0]\n",
    "print(\"accuracy = \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
